input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["health_checks_events"]
    group_id => "logstash_health_checks_consumer"
    auto_offset_reset => "earliest"
    consumer_threads => "3"
    codec => "json" 
  }
}

filter {
  elasticsearch {
    hosts => ["http://es:9200"]
    index => "health_checks"
    query => "server_id:%{[server_id]}"
    sort => "timestamp:desc"
    fields => { "timestamp" => "latest_doc_timestamp" }
  }
  if [latest_doc_timestamp] {
    ruby {
      code => "
        current_ts = Time.parse(event.get('timestamp'))
        latest_ts = Time.parse(event.get('latest_doc_timestamp'))

        time_diff_ms = ((current_ts.to_f - latest_ts.to_f) * 1000).to_i

        incoming_interval = event.get('interval_since_last_health_check_ms')

        final_interval = [incoming_interval, time_diff_ms].min

        event.set('interval_since_last_health_check_ms', final_interval)
      "
    }
  }

  mutate {
    remove_field => ["latest_doc_timestamp", "event", "@timestamp", "@version"]
    convert => {
      "status_numeric" => "integer"
      "attempts" => "integer"
      "interval_since_last_health_check_ms" => "integer"
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://es:9200"]
    index => "health_checks"
    document_id => "%{[server_id]}_%{[timestamp]}"
    action => "create"
  }
}